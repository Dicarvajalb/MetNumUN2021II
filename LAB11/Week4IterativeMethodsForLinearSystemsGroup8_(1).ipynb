{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dicarvajalb/MetNumUN2021II/blob/main/LAB11/Week4IterativeMethodsForLinearSystemsGroup8_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CGupB3cBPr7"
      },
      "source": [
        "# Simple iteration for systems of linear equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGyvCcklBPsA"
      },
      "source": [
        "First, generate a random diagonally dominant matrix, for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC_g8ispBPsB",
        "outputId": "32224510-2e21-41cc-8c5a-ea1714fa5e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.2  0.6  0.4  0.8  0.8  0.3  0.3  0.8  1.   0.9]\n",
            " [ 0.4 15.5  0.7  0.7  0.4  0.6  0.5  0.   0.8  0.9]\n",
            " [ 0.4  0.6 15.1  0.4  0.9  0.7  0.4  0.8  0.3  0.6]\n",
            " [ 0.9  0.4  0.8 15.1  0.7  0.7  0.2  0.9  0.4  0.9]\n",
            " [ 0.1  0.2  0.   0.7 15.6  0.5  0.   0.6  0.3  0.5]\n",
            " [ 0.1  0.6  0.6  0.   0.6 15.9  0.8  1.   1.   0.8]\n",
            " [ 0.3  0.6  0.5  0.2  0.4  0.1 15.5  1.   0.1  0.1]\n",
            " [ 0.7  0.6  0.5  0.1  0.2  0.9  0.4 15.5  0.   0.3]\n",
            " [ 0.4  0.6  0.9  0.6  0.7  0.1  0.7  0.8 15.6  0.4]\n",
            " [ 0.2  0.6  0.5  1.   0.5  0.5  0.5  0.8  0.1 15.7]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=1, suppress=True)\n",
        "rndm = np.random.RandomState(1234)\n",
        "\n",
        "n = 10\n",
        "A = rndm.uniform(size=(n, n)) + np.diagflat([15]*n)\n",
        "b = rndm.uniform(size=n)\n",
        "\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m8M1COmBPsE"
      },
      "source": [
        "# I.  Jacobi iteration\n",
        "\n",
        "Given\n",
        "\n",
        "$$\n",
        "A x = b\n",
        "$$\n",
        "\n",
        "separate the diagonal part $D$,\n",
        "\n",
        "$$ A = D + (A - D) $$\n",
        "\n",
        "and write\n",
        "\n",
        "$$\n",
        "x = D^{-1} (D - A) x + D^{-1} b\\;.\n",
        "$$\n",
        "\n",
        "Then iterate\n",
        "\n",
        "$$\n",
        "x_{n + 1} = B x_{n} + c\\;,\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "B = D^{-1} (A - D) \\qquad \\text{and} \\qquad c = D^{-1} b\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-7gVdKDBPsF"
      },
      "source": [
        "Let's construct the matrix and the r.h.s. for the Jacobi iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThdSbrfwBPsF"
      },
      "outputs": [],
      "source": [
        "diag_1d = np.diag(A)\n",
        "\n",
        "B = -A.copy() \n",
        "np.fill_diagonal(B, 0)   # A-D\n",
        "\n",
        "D = np.diag(diag_1d)\n",
        "invD = np.diag(1./diag_1d)\n",
        "BB = invD @ B \n",
        "c = invD @ b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgL1y50yBPsG"
      },
      "outputs": [],
      "source": [
        "# sanity checks\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "from numpy.testing import assert_allclose\n",
        "\n",
        "assert_allclose(-B + D, A)\n",
        "\n",
        "\n",
        "# xx is a \"ground truth\" solution, compute it using a direct method\n",
        "xx = np.linalg.solve(A, b)\n",
        "\n",
        "np.testing.assert_allclose(A@xx, b)\n",
        "np.testing.assert_allclose(D@xx, B@xx + b)\n",
        "np.testing.assert_allclose(xx, BB@xx + c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97HTCK6BPsG"
      },
      "source": [
        "Check that $\\| B\\| \\leqslant 1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlswuxWQBPsH",
        "outputId": "d9ee6826-d97e-40be-ec92-bed308508966"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36436161983015336"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "np.linalg.norm(BB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VPbC2GSBPsI"
      },
      "source": [
        "### Do the Jacobi iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaSC9dYABPsI",
        "outputId": "67c1ceb9-62cf-42c0-a236-b90093feeba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x =  [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n"
          ]
        }
      ],
      "source": [
        "n_iter = 5000\n",
        "\n",
        "x0 = np.ones(n)\n",
        "x = x0\n",
        "for _ in range(n_iter):\n",
        "    x = BB @ x + c\n",
        "print('x = ',x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVVigMZIBPsJ",
        "outputId": "8cd2425a-c17d-4d23-d1d9-ca8c15cff781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Check the result:\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "A @ x - b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuW0qhz4BPsJ"
      },
      "source": [
        "### Task I.1\n",
        "\n",
        "Collect the proof-of-concept above into a single function implementing the Jacobi iteration. This function should receive the r.h.s. matrix $A$, the l.h.s. vector `b`, and the number of iterations to perform.\n",
        "\n",
        "\n",
        "The matrix $A$ in the illustration above is strongly diagonally dominant, by construction. \n",
        "What happens if the diagonal matrix elements of $A$ are made smaller? Check the convergence of the Jacobi iteration, and check the value of the norm of $B$.\n",
        "\n",
        "(20% of the total grade)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-trYFq_BPsK"
      },
      "outputs": [],
      "source": [
        "def jacobi(A, b, n_iter=1000):\n",
        "    '''\n",
        "    Performs a Jacobi iteration for de Ax = b system\n",
        "    '''\n",
        "    diag_A = np.diag(A)        # Elements of the diagonal of A. \n",
        "                               # NOTE: np.diag --> Extract a diagonal or construct a diagonal array.\n",
        "        \n",
        "    BB = -A.copy()             # BB is the first B matrix\n",
        "    np.fill_diagonal(BB, 0)    # D-A\n",
        "    D = np.diag(diag_A)        # Diagonal matrix\n",
        "    invD = np.diag(1./diag_A)\n",
        "    B = invD @ BB              # This is the last B matrix\n",
        "    c = invD @ b\n",
        "    norm = np.linalg.norm(B)\n",
        "    \n",
        "    x0 = np.ones(A.shape[0])\n",
        "    x = x0\n",
        "    for _ in range(n_iter):\n",
        "        x = B @ x + c\n",
        "    \n",
        "    return x, norm\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYK4MYPTBPsK",
        "outputId": "5d92b5ee-e43d-4ebf-a9a9-e952aaa3dd61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.0392,  0.0378,  0.0428,  0.0237,  0.0575, -0.0003, -0.0058,\n",
              "         0.0318, -0.0042,  0.0528]), 0.36436161983015336)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "jacobi(A, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqgkyQGWBPsK",
        "outputId": "65de7b57-c3f9-499a-a858-e5aff3b02701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15.1915  0.6221  0.4377  0.7854  0.78    0.2726  0.2765  0.8019  0.9581\n",
            "   0.8759]\n",
            " [ 0.3578 15.501   0.6835  0.7127  0.3703  0.5612  0.5031  0.0138  0.7728\n",
            "   0.8826]\n",
            " [ 0.3649  0.6154 15.0754  0.3688  0.9331  0.6514  0.3972  0.7887  0.3168\n",
            "   0.5681]\n",
            " [ 0.8691  0.4362  0.8021 15.1438  0.7043  0.7046  0.2188  0.9249  0.4421\n",
            "   0.9093]\n",
            " [ 0.0598  0.1843  0.0474  0.6749 15.5946  0.5333  0.0433  0.5614  0.3297\n",
            "   0.503 ]\n",
            " [ 0.1119  0.6072  0.5659  0.0068  0.6174 15.9121  0.7905  0.9921  0.9588\n",
            "   0.792 ]\n",
            " [ 0.2853  0.6249  0.4781  0.1957  0.3823  0.0539 15.4516  0.982   0.1239\n",
            "   0.1194]\n",
            " [ 0.7385  0.5873  0.4716  0.1071  0.2292  0.9     0.4168 15.5359  0.0062\n",
            "   0.3006]\n",
            " [ 0.4369  0.6121  0.9182  0.6257  0.706   0.1498  0.7461  0.831  15.6337\n",
            "   0.4383]\n",
            " [ 0.1526  0.5684  0.5282  0.9514  0.4804  0.5026  0.5369  0.8192  0.0571\n",
            "  15.6694]]\n"
          ]
        }
      ],
      "source": [
        "rndm = np.random.RandomState(1234)\n",
        "\n",
        "n = 10\n",
        "A = rndm.uniform(size=(n, n)) + np.diagflat([15] * n)    # CHANGE THIS\n",
        "b = rndm.uniform(size=n)\n",
        "\n",
        "print(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhT8R5AUBPsL",
        "outputId": "c21ad2fc-c3a1-4b28-d172-1a0e2709d6a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.0392,  0.0378,  0.0428,  0.0237,  0.0575, -0.0003, -0.0058,\n",
              "         0.0318, -0.0042,  0.0528]), 0.36436161983015336)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "jacobi(A, b, 5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDC2MwU0BPsL"
      },
      "source": [
        "You can check convergence changing the value on the diagonal. When the norm is less than 1, it converges quickly. If is close to one, you need more iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRLcefW2BPsL"
      },
      "source": [
        "# II. Seidel's iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SpLKi11BPsM"
      },
      "source": [
        "##### Task II.1\n",
        "\n",
        "Implement the Seidel's iteration. \n",
        "\n",
        "Test it on a random matrix. Study the convergence of iterations, relate to the norm of the iteration matrix.\n",
        "\n",
        "(30% of the total grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi_gQYvFBPsM"
      },
      "outputs": [],
      "source": [
        "def seidel(A, b, n_iter=1000):\n",
        "    '''\n",
        "    Performs a Seidel's iteration for de Ax = b system\n",
        "    '''\n",
        "    diag_A = np.diag(A)        # Elements of the diagonal of A. \n",
        "                               # NOTE: np.diag --> Extract a diagonal or construct a diagonal array.\n",
        "    \n",
        "    D = np.diag(diag_A)        # Diagonal matrix\n",
        "    invD = np.diag(1./diag_A)\n",
        "    U = np.triu(A) - D\n",
        "    L = np.tril(A) - D\n",
        "    B_tilde = -np.linalg.inv(D+L) @ U \n",
        "    c_tilde = np.linalg.inv(D+L) @ b \n",
        "    \n",
        "    norm = np.linalg.norm(B)\n",
        "    \n",
        "    x0 = np.ones(A.shape[0])\n",
        "    x = x0\n",
        "    \n",
        "    for _ in range(n_iter):\n",
        "        x = B_tilde @ x + c_tilde\n",
        "    \n",
        "    return x, norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMn0O3SXBPsM",
        "outputId": "4b9c9057-e2b4-4d39-e49a-078769db68b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobi [-0.332 -0.268 -0.279 -0.36  -0.126 -0.333 -0.208 -0.204 -0.342 -0.233]\n",
            "Seidel [-0.332 -0.237 -0.196 -0.185 -0.052 -0.192 -0.051  0.064  0.02   0.096]\n",
            "\n",
            "Jacobi [0.15  0.135 0.135 0.139 0.117 0.092 0.054 0.106 0.094 0.138]\n",
            "Seidel [ 0.072  0.064  0.058  0.03   0.06  -0.006 -0.011  0.028 -0.008  0.051]\n",
            "\n",
            "Jacobi [ 0.006  0.009  0.015 -0.011  0.04  -0.028 -0.024  0.01  -0.034  0.027]\n",
            "Seidel [ 0.038  0.037  0.043  0.024  0.058  0.    -0.005  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.049  0.046  0.051  0.034  0.063  0.008 -0.     0.038  0.005  0.061]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.036  0.035  0.04   0.021  0.056 -0.003 -0.007  0.03  -0.007  0.051]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.04   0.039  0.044  0.025  0.058  0.    -0.005  0.032 -0.003  0.054]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.023  0.057 -0.001 -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n",
            "Jacobi [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "Seidel [ 0.039  0.038  0.043  0.024  0.057 -0.    -0.006  0.032 -0.004  0.053]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "iters = range(1,15)\n",
        "\n",
        "for i in iters:\n",
        "    print('Jacobi', jacobi(A, b, i)[0])\n",
        "    print('Seidel', seidel(A, b, i)[0])\n",
        "    print('')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGToxFqIBPsN"
      },
      "source": [
        "Seidel has a better convergence for the matrix A given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-AowaKjBPsN"
      },
      "source": [
        "# III. Minimum residual scheme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMIPc8OBPsN"
      },
      "source": [
        "### Task III.1\n",
        "\n",
        "Implement the $\\textit{minimum residual}$ scheme: an explicit non-stationary method, where at each step you select the iteration parameter $\\tau_n$ to minimize the residual $\\mathbf{r}_{n+1}$ given $\\mathbf{r}_n$. Test it on a random matrix, study the convergence to the solution, in terms of the norm of the residual and the deviation from the ground truth solution (which you can obtain using a direct method). Study how the iteration parameter $\\tau_n$ changes as iterations progress.\n",
        "\n",
        "(50% of the grade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMuEMM2QBPsN"
      },
      "outputs": [],
      "source": [
        "def minres(A, b, n_iter=500):\n",
        "    x = np.ones(b.shape[0])\n",
        "    T = []\n",
        "    for _ in range(n_iter):\n",
        "        r = A @ x - b\n",
        "        tau = (r @ A @ r)/np.linalg.norm(A @ r)**2\n",
        "        x = x - tau*r\n",
        "        T.append(tau)\n",
        "    return x, T\n",
        "\n",
        "x = minres(A, b)[0]\n",
        "\n",
        "np.testing.assert_allclose(A@x, b)\n",
        "np.testing.assert_allclose(x, xx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUGccJlJBPsO",
        "outputId": "4e231f91-a957-424b-8436-495bca49d09b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049458091915919704,\n",
              " 0.06632815899696641,\n",
              " 0.05646528371934232,\n",
              " 0.05913686144833422,\n",
              " 0.056149365850671325,\n",
              " 0.05962056630691854,\n",
              " 0.055321259622620605,\n",
              " 0.060191395429619744,\n",
              " 0.054726519624867076,\n",
              " 0.06091835471753334,\n",
              " 0.054788794256908566,\n",
              " 0.06154317282841526,\n",
              " 0.055177210974496134,\n",
              " 0.06162125644817705,\n",
              " 0.05514529510209265,\n",
              " 0.06170111966684044,\n",
              " 0.054725649763148955,\n",
              " 0.062120810199901635,\n",
              " 0.05400213360890566,\n",
              " 0.06437401757338991,\n",
              " 0.054433035643282154,\n",
              " 0.06102904151784673,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958,\n",
              " 0.06413230252109958]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "minres(A, b)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evidencias\n",
        "Diego Carvajal\n",
        "![](https://drive.google.com/uc?id=1Zvn9kUnVzodmZKmU09O3xtzUU52zyHnE)\n",
        "Nicolas Hoyos\n",
        "![](https://drive.google.com/uc?id=1FXikcX0xiVs4cWiwKvfFOzZ500LlsScK)"
      ],
      "metadata": {
        "id": "NRVnlODMf0DT"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Week4IterativeMethodsForLinearSystemsGroup8 (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}